{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task 1.1\n",
    "Read the file soc-redditHyperlinks-title.tsv once \n",
    "and create a tab-separated dictionary file dict.tsv with two columns. \n",
    "\n",
    "The second column should include \n",
    "    the sorted distinct set of distinct names of subreddits\n",
    "     that appear either as SOURCE_SUBREDDIT or as TARGET_SUBREDDIT. \n",
    "The first column gives a unique integer identifier to each name, \n",
    "    based on their order, starting from 0. \n",
    "'''\n",
    "import pandas as pd\n",
    "filename = \"soc-redditHyperlinks-title.tsv\"\n",
    "df = pd.read_csv(filename, sep='\\t')\n",
    "\n",
    "subreddits = set(df['SOURCE_SUBREDDIT'].unique()).union(set(df['TARGET_SUBREDDIT'].unique()))\n",
    "subreddits = list(subreddits)\n",
    "subreddits.sort()\n",
    "# print(subreddits)\n",
    "# convert the subreddits to a df, with a unique integer identifier\n",
    "subreddit_df = pd.DataFrame(subreddits, columns=['subreddit'])\n",
    "subreddit_df['id'] = subreddit_df.index\n",
    "subreddit_df = subreddit_df[['id', 'subreddit']]\n",
    "subreddit_df.to_csv('dict.tsv', sep='\\t', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               subreddit   timestamp   destination_id emotion\n",
      "0                    007  1456025511     daniel_craig       1\n",
      "1          07thexpansion  1399709325     visualnovels       1\n",
      "2       098f6bcd4621d373  1486537717        askreddit       1\n",
      "3             0________0  1477368034    todayilearned       1\n",
      "4                0magick  1482323500           occult       1\n",
      "...                  ...         ...              ...     ...\n",
      "562627         zyramains  1483350482  leagueoflegends       1\n",
      "562628         zyramains  1486003374  leagueoflegends       1\n",
      "562629         zyramains  1491027357   summonerschool       1\n",
      "562630              zyzz  1401737952     bodybuilding       1\n",
      "562631              zyzz  1465406908       girlsmirin       1\n",
      "\n",
      "[562632 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Task 1.2\n",
    "Read the file soc-redditHyperlinks-title.tsv once more and \n",
    "create a tab-separated graph file with adjacency lists graph.tsv as follows. \n",
    "\n",
    "    Each line of graph.tsv should contain a subreddit identifier \n",
    "     that has at least one outgoing edge in soc-redditHyperlinks-title.tsv. \n",
    "    Next to the identifier, after a tab, there should be space-separated triples of the format: \n",
    "     timestamp, destination-id, sentiment, one for each outgoing interaction\n",
    "     from the source subreddit to the other (target) subreddits. \n",
    "      Timestamp is the unix epoch time that corresponds to the timestamp of the interaction and it is sensitive to the timezone. \n",
    "       For example, in the timezone of Hong Kong, timestamp 1456025511 corresponds to Sunday, 21 February 2016 11:31:51 GMT+08:00. \n",
    "    For each line of graph.tsv, the outgoing interactions should be sorted \n",
    "     (primarily by timestamp, secondarily by target subreddit) and \n",
    "     duplicate interactions should be eliminated.\n",
    "'''\n",
    "import pandas as pd\n",
    "import pytz\n",
    "\n",
    "df = pd.read_csv(filename, sep='\\t')\n",
    "graph_df = pd.DataFrame(columns=['subreddit', 'timestamp', 'destination_id', 'emotion'])\n",
    "\n",
    "\n",
    "\n",
    "def process_group(group):\n",
    "    outgoing_edges = group[['TIMESTAMP', 'TARGET_SUBREDDIT', 'LINK_SENTIMENT']]\n",
    "    outgoing_edges = outgoing_edges.drop_duplicates()\n",
    "    outgoing_edges.columns = ['timestamp', 'destination_id', 'emotion']\n",
    "    \n",
    "    outgoing_edges['timestamp'] = outgoing_edges['timestamp'].apply(lambda x: pd.Timestamp(x).timestamp())\n",
    "    outgoing_edges.insert(0, 'subreddit', group.name)\n",
    "    return outgoing_edges\n",
    "\n",
    "grouped = df.groupby('SOURCE_SUBREDDIT')\n",
    "graph_df = pd.concat([graph_df, grouped.apply(process_group)], ignore_index=True)\n",
    "\n",
    "# Convert to hk zone\n",
    "graph_df['timestamp'] = graph_df['timestamp'] - 28800\n",
    "# Convert to string\n",
    "graph_df['timestamp'] = graph_df['timestamp'].apply(lambda x: str(int(x)))\n",
    "\n",
    "print(graph_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        subreddit   timestamp  destination_id emotion\n",
      "0               0  1456025511           11584       1\n",
      "1               1  1399709325           51374       1\n",
      "2               3  1486537717            3242       1\n",
      "3               4  1477368034           48212       1\n",
      "4               5  1482323500           32872       1\n",
      "...           ...         ...             ...     ...\n",
      "562627      54072  1483350482           25860       1\n",
      "562628      54072  1486003374           25860       1\n",
      "562629      54072  1491027357           44951       1\n",
      "562630      54073  1401737952            6242       1\n",
      "562631      54073  1465406908           19155       1\n",
      "\n",
      "[562632 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the dict.tsv file and create a dictionary mapping subreddit names to subreddit ids\n",
    "dict_df = pd.read_csv('dict.tsv', sep='\\t', header=None, names=['id', 'subreddit'])\n",
    "subreddit_dict = dict(zip(dict_df['subreddit'], dict_df['id']))\n",
    "\n",
    "# Convert the data type of subreddit and destination_id columns to match the data type of the keys in subreddit_dict\n",
    "graph_df['subreddit'] = graph_df['subreddit'].astype(str)\n",
    "graph_df['destination_id'] = graph_df['destination_id'].astype(str)\n",
    "\n",
    "# Replace the subreddit names with the subreddit ids, and fill NaN values with -1\n",
    "graph_df['subreddit'] = graph_df['subreddit'].map(subreddit_dict).fillna(-1)\n",
    "graph_df['destination_id'] = graph_df['destination_id'].map(subreddit_dict).fillna(-1)\n",
    "\n",
    "print(graph_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['subreddit', 'timestamp', 'destination_id', 'emotion'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(graph_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency_list(graph_df, output_file):\n",
    "    adjacency_list = {}\n",
    "\n",
    "    # 构建邻接表\n",
    "    for _, row in graph_df.iterrows():\n",
    "        subreddit = str(row['subreddit'])\n",
    "        destination_id = str(row['destination_id'])\n",
    "        timestamp = str(row['timestamp'])\n",
    "        emotion = str(row['emotion'])\n",
    "        \n",
    "        if subreddit not in adjacency_list:\n",
    "            adjacency_list[subreddit] = []\n",
    "        \n",
    "        adjacency_list[subreddit].append(( timestamp, destination_id,emotion))\n",
    "    \n",
    "    # 写入邻接表到文件\n",
    "    with open(output_file, 'w') as f:\n",
    "        for subreddit, edges in adjacency_list.items():\n",
    "            destinations = '\\t'.join(f\"{timestamp},{dest_id},{emotion}\" for timestamp, dest_id, emotion in edges)\n",
    "            line = f\"{subreddit}\\t{destinations}\\n\"\n",
    "            f.write(line)\n",
    "\n",
    "create_adjacency_list(graph_df, 'graph.tsv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Plotly",
   "language": "python",
   "name": "Plotly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
